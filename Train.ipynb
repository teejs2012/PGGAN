{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch,os,time\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms,datasets\n",
    "import numpy as np\n",
    "from utils.data import RandomNoiseGenerator,Data\n",
    "from utils.train_history import train_history\n",
    "from utils import visualizer,util\n",
    "import itertools\n",
    "from models.model import Generator, Discriminator\n",
    "from IPython.core.debugger import set_trace\n",
    "# from models.model import Generator_test, Discriminator_test\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=3)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda:3' if torch.cuda.is_available() else 'cpu')\n",
    "if torch.backends.cudnn.enabled:\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_size = 512\n",
    "target_resol = 256\n",
    "first_resol = 32\n",
    "use_sigmoid = False\n",
    "train_kimg = 600\n",
    "train_img = train_kimg*1000\n",
    "transition_kimg = 600\n",
    "transition_img = transition_kimg*1000\n",
    "\n",
    "g_lr_max =  0.0002\n",
    "d_lr_max =  0.0002\n",
    "\n",
    "beta1 = 0.5\n",
    "beta2 = 0.999\n",
    "\n",
    "lambda_A=10\n",
    "lambda_B=10\n",
    "lambda_recon=3\n",
    "lambda_idt=1\n",
    "\n",
    "report_it = 400\n",
    "show_it = 400\n",
    "save_it=400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create web directory results/pggan2/web...\n"
     ]
    }
   ],
   "source": [
    "display_id=1\n",
    "display_winsize=256\n",
    "display_ncols=3\n",
    "display_server='http://localhost'\n",
    "display_port=8097\n",
    "display_env='pggan2'\n",
    "results_dir='results'\n",
    "project_name = 'pggan2'\n",
    "project_dir=os.path.join(results_dir,project_name)\n",
    "\n",
    "vis = visualizer.Visualizer(display_id,display_winsize,display_ncols,display_server,display_port,display_env,\n",
    "                 project_name,results_dir)\n",
    "\n",
    "if not os.path.isdir(project_dir):\n",
    "    os.makedirs(project_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44476\n",
      "35991\n"
     ]
    }
   ],
   "source": [
    "# real_dir = '/data/persona_cyclegan/faces_wo_bg/train'\n",
    "anime_dir = '/data/persona_cyclegan/anime_mix_bg'\n",
    "# anime_dir = '/data/persona_cyclegan/anime/trainB'\n",
    "real_dir = '/data/persona_cyclegan/real_mix_face_2'\n",
    "\n",
    "# real_dir = '/data/persona_cyclegan/real_test'\n",
    "# anime_dir = '/data/persona_cyclegan/anime_test'\n",
    "data_A = Data(real_dir)\n",
    "data_B = Data(anime_dir)\n",
    "print(data_A.get_count())\n",
    "print(data_B.get_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# real_dir = '/data/persona_cyclegan/faces_wo_bg'\n",
    "# anime_dir = '/data/persona_cyclegan/anime'\n",
    "# def load_imgs(src_data_path, tgt_data_path, fine_size):\n",
    "# #     load_size = fine_size * 9//8\n",
    "#     transform = transforms.Compose([\n",
    "#             transforms.Resize((fine_size, fine_size)),\n",
    "# #             transforms.RandomCrop(fine_size),\n",
    "#             transforms.RandomHorizontalFlip(),\n",
    "#             transforms.ColorJitter(0.1,0.1,0.1,0.1),\n",
    "#             transforms.ToTensor(),\n",
    "#             transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
    "#     ])\n",
    "    \n",
    "#     train_loader_A = torch.utils.data.DataLoader(datasets.ImageFolder(src_data_path, transform), batch_size=32, shuffle=True, drop_last=True)\n",
    "#     train_loader_B = torch.utils.data.DataLoader(datasets.ImageFolder(tgt_data_path, transform), batch_size=32, shuffle=True, drop_last=True)\n",
    "#     return train_loader_A,train_loader_B\n",
    "\n",
    "# train_loader_A,train_loader_B = load_imgs(real_dir, anime_dir, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator(\n",
      "  (output_layer): GSelectLayer(\n",
      "    (pre): ModuleList(\n",
      "      (0): Sequential(\n",
      "        (0): ReflectionPad2d((3, 3, 3, 3))\n",
      "        (1): Conv2d(3, 512, kernel_size=(7, 7), stride=(1, 1))\n",
      "        (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (3): ReLU(inplace)\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        (0): ReflectionPad2d((3, 3, 3, 3))\n",
      "        (1): Conv2d(3, 512, kernel_size=(7, 7), stride=(1, 1))\n",
      "        (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (3): ReLU(inplace)\n",
      "      )\n",
      "      (2): Sequential(\n",
      "        (0): ReflectionPad2d((3, 3, 3, 3))\n",
      "        (1): Conv2d(3, 512, kernel_size=(7, 7), stride=(1, 1))\n",
      "        (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (3): ReLU(inplace)\n",
      "      )\n",
      "      (3): Sequential(\n",
      "        (0): ReflectionPad2d((3, 3, 3, 3))\n",
      "        (1): Conv2d(3, 512, kernel_size=(7, 7), stride=(1, 1))\n",
      "        (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (3): ReLU(inplace)\n",
      "      )\n",
      "      (4): Sequential(\n",
      "        (0): ReflectionPad2d((3, 3, 3, 3))\n",
      "        (1): Conv2d(3, 256, kernel_size=(7, 7), stride=(1, 1))\n",
      "        (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (3): ReLU(inplace)\n",
      "      )\n",
      "      (5): Sequential(\n",
      "        (0): ReflectionPad2d((3, 3, 3, 3))\n",
      "        (1): Conv2d(3, 128, kernel_size=(7, 7), stride=(1, 1))\n",
      "        (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (3): ReLU(inplace)\n",
      "      )\n",
      "      (6): Sequential(\n",
      "        (0): ReflectionPad2d((3, 3, 3, 3))\n",
      "        (1): Conv2d(3, 64, kernel_size=(7, 7), stride=(1, 1))\n",
      "        (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (3): ReLU(inplace)\n",
      "      )\n",
      "    )\n",
      "    (en_chain): ModuleList(\n",
      "      (0): Sequential(\n",
      "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "        (1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (3): ReLU(inplace)\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "        (1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (3): ReLU(inplace)\n",
      "      )\n",
      "      (2): Sequential(\n",
      "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "        (1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (3): ReLU(inplace)\n",
      "      )\n",
      "      (3): Sequential(\n",
      "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "        (1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (3): ReLU(inplace)\n",
      "      )\n",
      "      (4): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "        (1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (3): ReLU(inplace)\n",
      "      )\n",
      "      (5): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (3): ReLU(inplace)\n",
      "      )\n",
      "      (6): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "        (1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (3): ReLU(inplace)\n",
      "      )\n",
      "    )\n",
      "    (de_chain): ModuleList(\n",
      "      (0): Sequential(\n",
      "        (0): ConvTranspose2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "        (1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (3): ReLU(inplace)\n",
      "        (4): resnet(\n",
      "          (conv_block): Sequential(\n",
      "            (0): ReflectionPad2d((1, 1, 1, 1))\n",
      "            (1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1))\n",
      "            (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "            (3): ReLU(inplace)\n",
      "            (4): ReflectionPad2d((1, 1, 1, 1))\n",
      "            (5): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1))\n",
      "            (6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        (0): ConvTranspose2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "        (1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (3): ReLU(inplace)\n",
      "        (4): resnet(\n",
      "          (conv_block): Sequential(\n",
      "            (0): ReflectionPad2d((1, 1, 1, 1))\n",
      "            (1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1))\n",
      "            (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "            (3): ReLU(inplace)\n",
      "            (4): ReflectionPad2d((1, 1, 1, 1))\n",
      "            (5): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1))\n",
      "            (6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): Sequential(\n",
      "        (0): ConvTranspose2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "        (1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (3): ReLU(inplace)\n",
      "        (4): resnet(\n",
      "          (conv_block): Sequential(\n",
      "            (0): ReflectionPad2d((1, 1, 1, 1))\n",
      "            (1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1))\n",
      "            (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "            (3): ReLU(inplace)\n",
      "            (4): ReflectionPad2d((1, 1, 1, 1))\n",
      "            (5): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1))\n",
      "            (6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (3): Sequential(\n",
      "        (0): ConvTranspose2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "        (1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (3): ReLU(inplace)\n",
      "        (4): resnet(\n",
      "          (conv_block): Sequential(\n",
      "            (0): ReflectionPad2d((1, 1, 1, 1))\n",
      "            (1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1))\n",
      "            (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "            (3): ReLU(inplace)\n",
      "            (4): ReflectionPad2d((1, 1, 1, 1))\n",
      "            (5): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1))\n",
      "            (6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (4): Sequential(\n",
      "        (0): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (3): ReLU(inplace)\n",
      "        (4): resnet(\n",
      "          (conv_block): Sequential(\n",
      "            (0): ReflectionPad2d((1, 1, 1, 1))\n",
      "            (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "            (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "            (3): ReLU(inplace)\n",
      "            (4): ReflectionPad2d((1, 1, 1, 1))\n",
      "            (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "            (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (5): Sequential(\n",
      "        (0): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "        (1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (3): ReLU(inplace)\n",
      "        (4): resnet(\n",
      "          (conv_block): Sequential(\n",
      "            (0): ReflectionPad2d((1, 1, 1, 1))\n",
      "            (1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "            (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "            (3): ReLU(inplace)\n",
      "            (4): ReflectionPad2d((1, 1, 1, 1))\n",
      "            (5): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "            (6): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (6): Sequential(\n",
      "        (0): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "        (1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (3): ReLU(inplace)\n",
      "        (4): resnet(\n",
      "          (conv_block): Sequential(\n",
      "            (0): ReflectionPad2d((1, 1, 1, 1))\n",
      "            (1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "            (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "            (3): ReLU(inplace)\n",
      "            (4): ReflectionPad2d((1, 1, 1, 1))\n",
      "            (5): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "            (6): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (post): ModuleList(\n",
      "      (0): Sequential(\n",
      "        (0): ReflectionPad2d((3, 3, 3, 3))\n",
      "        (1): Conv2d(512, 3, kernel_size=(7, 7), stride=(1, 1))\n",
      "        (2): Tanh()\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        (0): ReflectionPad2d((3, 3, 3, 3))\n",
      "        (1): Conv2d(512, 3, kernel_size=(7, 7), stride=(1, 1))\n",
      "        (2): Tanh()\n",
      "      )\n",
      "      (2): Sequential(\n",
      "        (0): ReflectionPad2d((3, 3, 3, 3))\n",
      "        (1): Conv2d(512, 3, kernel_size=(7, 7), stride=(1, 1))\n",
      "        (2): Tanh()\n",
      "      )\n",
      "      (3): Sequential(\n",
      "        (0): ReflectionPad2d((3, 3, 3, 3))\n",
      "        (1): Conv2d(512, 3, kernel_size=(7, 7), stride=(1, 1))\n",
      "        (2): Tanh()\n",
      "      )\n",
      "      (4): Sequential(\n",
      "        (0): ReflectionPad2d((3, 3, 3, 3))\n",
      "        (1): Conv2d(256, 3, kernel_size=(7, 7), stride=(1, 1))\n",
      "        (2): Tanh()\n",
      "      )\n",
      "      (5): Sequential(\n",
      "        (0): ReflectionPad2d((3, 3, 3, 3))\n",
      "        (1): Conv2d(128, 3, kernel_size=(7, 7), stride=(1, 1))\n",
      "        (2): Tanh()\n",
      "      )\n",
      "      (6): Sequential(\n",
      "        (0): ReflectionPad2d((3, 3, 3, 3))\n",
      "        (1): Conv2d(64, 3, kernel_size=(7, 7), stride=(1, 1))\n",
      "        (2): Tanh()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Discriminator(\n",
      "  (output_layer): DSelectLayer(\n",
      "    (chain): ModuleList(\n",
      "      (0): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (2): LeakyReLU(negative_slope=0.2, inplace)\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (2): LeakyReLU(negative_slope=0.2, inplace)\n",
      "      )\n",
      "      (2): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (2): LeakyReLU(negative_slope=0.2, inplace)\n",
      "      )\n",
      "      (3): Sequential(\n",
      "        (0): Conv2d(512, 512, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (2): LeakyReLU(negative_slope=0.2, inplace)\n",
      "      )\n",
      "      (4): Sequential(\n",
      "        (0): Conv2d(512, 512, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (2): LeakyReLU(negative_slope=0.2, inplace)\n",
      "      )\n",
      "      (5): Sequential(\n",
      "        (0): Conv2d(512, 512, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (2): LeakyReLU(negative_slope=0.2, inplace)\n",
      "      )\n",
      "      (6): Sequential(\n",
      "        (0): Conv2d(512, 512, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (2): LeakyReLU(negative_slope=0.2, inplace)\n",
      "        (3): Dropout(p=0.4)\n",
      "        (4): Conv2d(512, 1, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "      )\n",
      "    )\n",
      "    (inputs): ModuleList(\n",
      "      (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
      "      (1): Conv2d(3, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
      "      (2): Conv2d(3, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
      "      (3): Conv2d(3, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
      "      (4): Conv2d(3, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
      "      (5): Conv2d(3, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
      "      (6): Conv2d(3, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Networks initialized -------------\n",
      "G has 65890901 number of parameters\n",
      "D has 30900353 number of parameters\n",
      "-----------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "G_A = Generator(num_channels=3, latent_size=latent_size, resolution=target_resol, fmap_max=latent_size, fmap_base=8192, tanh_at_end=True,resnets=1)\n",
    "G_B = Generator(num_channels=3, latent_size=latent_size, resolution=target_resol, fmap_max=latent_size, fmap_base=8192, tanh_at_end=True,resnets=1)\n",
    "# G_A = Generator_test()\n",
    "# G_B = Generator_test()\n",
    "\n",
    "D_A = Discriminator(num_channels=3, mbstat_avg='all', resolution=target_resol, fmap_max=latent_size, fmap_base=8192, sigmoid_at_end=False)\n",
    "D_B = Discriminator(num_channels=3, mbstat_avg='all', resolution=target_resol, fmap_max=latent_size, fmap_base=8192, sigmoid_at_end=False)\n",
    "# D_A = Discriminator_test()\n",
    "# D_B = Discriminator_test()\n",
    "\n",
    "print(G_A)\n",
    "print(D_A)\n",
    "G_A,G_B,D_A,D_B = G_A.to(device),G_B.to(device),D_A.to(device),D_B.to(device)\n",
    "optim_G = optim.Adam(itertools.chain(G_A.parameters(),G_B.parameters()), g_lr_max, betas=(beta1, beta2))\n",
    "optim_D_A = optim.Adam(D_A.parameters(), d_lr_max, betas=(beta1, beta2))\n",
    "optim_D_B = optim.Adam(D_B.parameters(), d_lr_max, betas=(beta1, beta2))\n",
    "\n",
    "all_models = {'G_A.pkl':G_A,\n",
    "             'G_B.pkl':G_B,\n",
    "             'D_A.pkl':D_A,\n",
    "             'D_B.pkl':D_B}\n",
    "            \n",
    "\n",
    "print('---------- Networks initialized -------------')\n",
    "for model_name,model in [('G',G_A),('D',D_A)]:\n",
    "    num_params = 0\n",
    "    for param in model.parameters():\n",
    "        num_params += param.numel()\n",
    "    print(str.format('{} has {} number of parameters', model_name, num_params))\n",
    "print('-----------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rampup_kimg = 10000\n",
    "rampdown_kimg = 10000\n",
    "total_kimg = 10000\n",
    "\n",
    "def _rampup(epoch, rampup_length):\n",
    "    if epoch < rampup_length:\n",
    "        p = max(0.0, float(epoch)) / float(rampup_length)\n",
    "        p = 1.0 - p\n",
    "        return np.exp(-p*p*5.0)\n",
    "    else:\n",
    "        return 1.0\n",
    "\n",
    "def _rampdown_linear(epoch, num_epochs, rampdown_length):\n",
    "    if epoch >= num_epochs - rampdown_length:\n",
    "        return float(num_epochs - epoch) / rampdown_length\n",
    "    else:\n",
    "        return 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bs(resolution):\n",
    "    R = int(np.log2(resolution))\n",
    "    if R < 7:\n",
    "        bs = 32 / 2**(max(0, R-2))\n",
    "    else:\n",
    "        bs = 8 / 2**(min(3, R-5))\n",
    "    return int(bs)\n",
    "\n",
    "bs_map = {2**R: get_bs(2**R) for R in range(2, 11)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_hist = train_history(['G_gan_loss',\n",
    "                              'G_idt_loss',\n",
    "                              'G_cycle_loss',\n",
    "                              'D_A_real_loss',\n",
    "                              'D_A_fake_loss',\n",
    "                              'D_B_real_loss',                                         \n",
    "                              'D_B_fake_loss',                                         \n",
    "                              ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L1_loss = nn.L1Loss().to(device) \n",
    "GAN_loss = nn.MSELoss().to(device)\n",
    "# GAN_loss = nn.BCELoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_models(models, folder):\n",
    "    for k, v in models.items():\n",
    "        torch.save(v.state_dict(), os.path.join(folder, k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models = {'G_A.pkl':G_A,\n",
    "             'G_B.pkl':G_B,\n",
    "             'D_A.pkl':D_A,\n",
    "             'D_B.pkl':D_B}\n",
    "\n",
    "def load_models(models, folder):\n",
    "    for k, v in models.items():\n",
    "        v.load_state_dict(torch.load(os.path.join(folder, k)))\n",
    "        \n",
    "load_models(all_models, os.path.join(results_dir,project_name,'fade_in3'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting scale 4, batch_size is 4, and the iterations in each phase is 150000\n",
      "(current_level: 4.000, iters: 0, time_from_last_report: 4.861, since_the_phase_start: 4.807) G_gan_loss: 1.937 G_idt_loss: 1.125 G_cycle_loss: 1.597 D_A_real_loss: 0.091 D_A_fake_loss: 0.197 D_B_real_loss: 0.877 D_B_fake_loss: 0.147 \n",
      "{'pre_used': [3], 'enchain_used': [3, 2, 1, 0], 'dechain_used': [0, 1, 2, 3], 'post_used': [3]}\n",
      "{'inputs_used': [3], 'chain_used': [3, 4, 5, 6]}\n",
      "(current_level: 4.000, iters: 400, time_from_last_report: 334.473, since_the_phase_start: 338.975) G_gan_loss: 1.131 G_idt_loss: 0.962 G_cycle_loss: 1.435 D_A_real_loss: 0.114 D_A_fake_loss: 0.115 D_B_real_loss: 0.190 D_B_fake_loss: 0.189 \n",
      "{'pre_used': [3], 'enchain_used': [3, 2, 1, 0], 'dechain_used': [0, 1, 2, 3], 'post_used': [3]}\n",
      "{'inputs_used': [3], 'chain_used': [3, 4, 5, 6]}\n",
      "(current_level: 4.000, iters: 800, time_from_last_report: 334.852, since_the_phase_start: 673.835) G_gan_loss: 1.099 G_idt_loss: 1.001 G_cycle_loss: 1.398 D_A_real_loss: 0.120 D_A_fake_loss: 0.120 D_B_real_loss: 0.187 D_B_fake_loss: 0.189 \n",
      "{'pre_used': [3], 'enchain_used': [3, 2, 1, 0], 'dechain_used': [0, 1, 2, 3], 'post_used': [3]}\n",
      "{'inputs_used': [3], 'chain_used': [3, 4, 5, 6]}\n",
      "(current_level: 4.000, iters: 1200, time_from_last_report: 334.340, since_the_phase_start: 1008.220) G_gan_loss: 1.046 G_idt_loss: 0.962 G_cycle_loss: 1.390 D_A_real_loss: 0.120 D_A_fake_loss: 0.130 D_B_real_loss: 0.192 D_B_fake_loss: 0.185 \n",
      "{'pre_used': [3], 'enchain_used': [3, 2, 1, 0], 'dechain_used': [0, 1, 2, 3], 'post_used': [3]}\n",
      "{'inputs_used': [3], 'chain_used': [3, 4, 5, 6]}\n",
      "(current_level: 4.000, iters: 1600, time_from_last_report: 333.871, since_the_phase_start: 1342.063) G_gan_loss: 1.086 G_idt_loss: 0.997 G_cycle_loss: 1.397 D_A_real_loss: 0.105 D_A_fake_loss: 0.109 D_B_real_loss: 0.183 D_B_fake_loss: 0.176 \n",
      "{'pre_used': [3], 'enchain_used': [3, 2, 1, 0], 'dechain_used': [0, 1, 2, 3], 'post_used': [3]}\n",
      "{'inputs_used': [3], 'chain_used': [3, 4, 5, 6]}\n",
      "(current_level: 4.000, iters: 2000, time_from_last_report: 334.607, since_the_phase_start: 1676.665) G_gan_loss: 1.383 G_idt_loss: 0.932 G_cycle_loss: 1.415 D_A_real_loss: 0.049 D_A_fake_loss: 0.059 D_B_real_loss: 0.189 D_B_fake_loss: 0.182 \n",
      "{'pre_used': [3], 'enchain_used': [3, 2, 1, 0], 'dechain_used': [0, 1, 2, 3], 'post_used': [3]}\n",
      "{'inputs_used': [3], 'chain_used': [3, 4, 5, 6]}\n",
      "(current_level: 4.000, iters: 2400, time_from_last_report: 334.105, since_the_phase_start: 2010.806) G_gan_loss: 1.267 G_idt_loss: 0.946 G_cycle_loss: 1.494 D_A_real_loss: 0.055 D_A_fake_loss: 0.056 D_B_real_loss: 0.185 D_B_fake_loss: 0.185 \n",
      "{'pre_used': [3], 'enchain_used': [3, 2, 1, 0], 'dechain_used': [0, 1, 2, 3], 'post_used': [3]}\n",
      "{'inputs_used': [3], 'chain_used': [3, 4, 5, 6]}\n",
      "(current_level: 4.000, iters: 2800, time_from_last_report: 334.759, since_the_phase_start: 2345.581) G_gan_loss: 1.150 G_idt_loss: 0.956 G_cycle_loss: 1.403 D_A_real_loss: 0.082 D_A_fake_loss: 0.085 D_B_real_loss: 0.189 D_B_fake_loss: 0.189 \n",
      "{'pre_used': [3], 'enchain_used': [3, 2, 1, 0], 'dechain_used': [0, 1, 2, 3], 'post_used': [3]}\n",
      "{'inputs_used': [3], 'chain_used': [3, 4, 5, 6]}\n",
      "(current_level: 4.000, iters: 3200, time_from_last_report: 333.540, since_the_phase_start: 2679.126) G_gan_loss: 1.120 G_idt_loss: 0.965 G_cycle_loss: 1.378 D_A_real_loss: 0.096 D_A_fake_loss: 0.101 D_B_real_loss: 0.191 D_B_fake_loss: 0.185 \n",
      "{'pre_used': [3], 'enchain_used': [3, 2, 1, 0], 'dechain_used': [0, 1, 2, 3], 'post_used': [3]}\n",
      "{'inputs_used': [3], 'chain_used': [3, 4, 5, 6]}\n",
      "(current_level: 4.000, iters: 3600, time_from_last_report: 333.438, since_the_phase_start: 3012.572) G_gan_loss: 1.097 G_idt_loss: 0.966 G_cycle_loss: 1.361 D_A_real_loss: 0.108 D_A_fake_loss: 0.114 D_B_real_loss: 0.193 D_B_fake_loss: 0.190 \n",
      "{'pre_used': [3], 'enchain_used': [3, 2, 1, 0], 'dechain_used': [0, 1, 2, 3], 'post_used': [3]}\n",
      "{'inputs_used': [3], 'chain_used': [3, 4, 5, 6]}\n",
      "(current_level: 4.000, iters: 4000, time_from_last_report: 333.968, since_the_phase_start: 3346.540) G_gan_loss: 1.094 G_idt_loss: 0.976 G_cycle_loss: 1.378 D_A_real_loss: 0.114 D_A_fake_loss: 0.110 D_B_real_loss: 0.193 D_B_fake_loss: 0.184 \n",
      "{'pre_used': [3], 'enchain_used': [3, 2, 1, 0], 'dechain_used': [0, 1, 2, 3], 'post_used': [3]}\n",
      "{'inputs_used': [3], 'chain_used': [3, 4, 5, 6]}\n",
      "(current_level: 4.000, iters: 4400, time_from_last_report: 333.549, since_the_phase_start: 3680.093) G_gan_loss: 1.102 G_idt_loss: 0.970 G_cycle_loss: 1.366 D_A_real_loss: 0.114 D_A_fake_loss: 0.122 D_B_real_loss: 0.188 D_B_fake_loss: 0.185 \n",
      "{'pre_used': [3], 'enchain_used': [3, 2, 1, 0], 'dechain_used': [0, 1, 2, 3], 'post_used': [3]}\n",
      "{'inputs_used': [3], 'chain_used': [3, 4, 5, 6]}\n",
      "(current_level: 4.000, iters: 4800, time_from_last_report: 333.548, since_the_phase_start: 4013.608) G_gan_loss: 1.126 G_idt_loss: 0.983 G_cycle_loss: 1.399 D_A_real_loss: 0.113 D_A_fake_loss: 0.115 D_B_real_loss: 0.180 D_B_fake_loss: 0.179 \n",
      "{'pre_used': [3], 'enchain_used': [3, 2, 1, 0], 'dechain_used': [0, 1, 2, 3], 'post_used': [3]}\n",
      "{'inputs_used': [3], 'chain_used': [3, 4, 5, 6]}\n",
      "(current_level: 4.000, iters: 5200, time_from_last_report: 333.921, since_the_phase_start: 4347.577) G_gan_loss: 1.109 G_idt_loss: 0.978 G_cycle_loss: 1.373 D_A_real_loss: 0.116 D_A_fake_loss: 0.119 D_B_real_loss: 0.185 D_B_fake_loss: 0.177 \n",
      "{'pre_used': [3], 'enchain_used': [3, 2, 1, 0], 'dechain_used': [0, 1, 2, 3], 'post_used': [3]}\n",
      "{'inputs_used': [3], 'chain_used': [3, 4, 5, 6]}\n",
      "(current_level: 4.000, iters: 5600, time_from_last_report: 333.727, since_the_phase_start: 4681.316) G_gan_loss: 1.332 G_idt_loss: 0.957 G_cycle_loss: 1.461 D_A_real_loss: 0.058 D_A_fake_loss: 0.060 D_B_real_loss: 0.173 D_B_fake_loss: 0.166 \n",
      "{'pre_used': [3], 'enchain_used': [3, 2, 1, 0], 'dechain_used': [0, 1, 2, 3], 'post_used': [3]}\n",
      "{'inputs_used': [3], 'chain_used': [3, 4, 5, 6]}\n",
      "(current_level: 4.000, iters: 6000, time_from_last_report: 333.160, since_the_phase_start: 5014.484) G_gan_loss: 1.096 G_idt_loss: 0.959 G_cycle_loss: 1.353 D_A_real_loss: 0.100 D_A_fake_loss: 0.106 D_B_real_loss: 0.187 D_B_fake_loss: 0.183 \n",
      "{'pre_used': [3], 'enchain_used': [3, 2, 1, 0], 'dechain_used': [0, 1, 2, 3], 'post_used': [3]}\n",
      "{'inputs_used': [3], 'chain_used': [3, 4, 5, 6]}\n",
      "(current_level: 4.000, iters: 6400, time_from_last_report: 333.127, since_the_phase_start: 5347.616) G_gan_loss: 1.125 G_idt_loss: 0.967 G_cycle_loss: 1.335 D_A_real_loss: 0.118 D_A_fake_loss: 0.120 D_B_real_loss: 0.182 D_B_fake_loss: 0.180 \n",
      "{'pre_used': [3], 'enchain_used': [3, 2, 1, 0], 'dechain_used': [0, 1, 2, 3], 'post_used': [3]}\n",
      "{'inputs_used': [3], 'chain_used': [3, 4, 5, 6]}\n",
      "(current_level: 4.000, iters: 6800, time_from_last_report: 333.233, since_the_phase_start: 5680.841) G_gan_loss: 1.089 G_idt_loss: 0.955 G_cycle_loss: 1.347 D_A_real_loss: 0.113 D_A_fake_loss: 0.119 D_B_real_loss: 0.182 D_B_fake_loss: 0.178 \n",
      "{'pre_used': [3], 'enchain_used': [3, 2, 1, 0], 'dechain_used': [0, 1, 2, 3], 'post_used': [3]}\n",
      "{'inputs_used': [3], 'chain_used': [3, 4, 5, 6]}\n",
      "(current_level: 4.000, iters: 7200, time_from_last_report: 333.106, since_the_phase_start: 6013.972) G_gan_loss: 1.203 G_idt_loss: 0.967 G_cycle_loss: 1.438 D_A_real_loss: 0.094 D_A_fake_loss: 0.095 D_B_real_loss: 0.167 D_B_fake_loss: 0.162 \n",
      "{'pre_used': [3], 'enchain_used': [3, 2, 1, 0], 'dechain_used': [0, 1, 2, 3], 'post_used': [3]}\n",
      "{'inputs_used': [3], 'chain_used': [3, 4, 5, 6]}\n",
      "(current_level: 4.000, iters: 7600, time_from_last_report: 334.357, since_the_phase_start: 6348.296) G_gan_loss: 1.142 G_idt_loss: 0.962 G_cycle_loss: 1.347 D_A_real_loss: 0.097 D_A_fake_loss: 0.100 D_B_real_loss: 0.180 D_B_fake_loss: 0.182 \n",
      "{'pre_used': [3], 'enchain_used': [3, 2, 1, 0], 'dechain_used': [0, 1, 2, 3], 'post_used': [3]}\n",
      "{'inputs_used': [3], 'chain_used': [3, 4, 5, 6]}\n",
      "(current_level: 4.000, iters: 8000, time_from_last_report: 334.498, since_the_phase_start: 6682.801) G_gan_loss: 1.153 G_idt_loss: 0.960 G_cycle_loss: 1.331 D_A_real_loss: 0.111 D_A_fake_loss: 0.114 D_B_real_loss: 0.179 D_B_fake_loss: 0.183 \n",
      "{'pre_used': [3], 'enchain_used': [3, 2, 1, 0], 'dechain_used': [0, 1, 2, 3], 'post_used': [3]}\n",
      "{'inputs_used': [3], 'chain_used': [3, 4, 5, 6]}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(current_level: 4.000, iters: 8400, time_from_last_report: 334.605, since_the_phase_start: 7017.414) G_gan_loss: 1.203 G_idt_loss: 0.975 G_cycle_loss: 1.356 D_A_real_loss: 0.108 D_A_fake_loss: 0.106 D_B_real_loss: 0.187 D_B_fake_loss: 0.177 \n",
      "{'pre_used': [3], 'enchain_used': [3, 2, 1, 0], 'dechain_used': [0, 1, 2, 3], 'post_used': [3]}\n",
      "{'inputs_used': [3], 'chain_used': [3, 4, 5, 6]}\n",
      "(current_level: 4.000, iters: 8800, time_from_last_report: 334.453, since_the_phase_start: 7351.854) G_gan_loss: 1.211 G_idt_loss: 0.939 G_cycle_loss: 1.405 D_A_real_loss: 0.073 D_A_fake_loss: 0.078 D_B_real_loss: 0.196 D_B_fake_loss: 0.190 \n",
      "{'pre_used': [3], 'enchain_used': [3, 2, 1, 0], 'dechain_used': [0, 1, 2, 3], 'post_used': [3]}\n",
      "{'inputs_used': [3], 'chain_used': [3, 4, 5, 6]}\n",
      "(current_level: 4.000, iters: 9200, time_from_last_report: 334.506, since_the_phase_start: 7686.390) G_gan_loss: 1.122 G_idt_loss: 0.947 G_cycle_loss: 1.330 D_A_real_loss: 0.110 D_A_fake_loss: 0.114 D_B_real_loss: 0.195 D_B_fake_loss: 0.194 \n",
      "{'pre_used': [3], 'enchain_used': [3, 2, 1, 0], 'dechain_used': [0, 1, 2, 3], 'post_used': [3]}\n",
      "{'inputs_used': [3], 'chain_used': [3, 4, 5, 6]}\n",
      "(current_level: 4.000, iters: 9600, time_from_last_report: 334.752, since_the_phase_start: 8021.150) G_gan_loss: 1.100 G_idt_loss: 0.960 G_cycle_loss: 1.329 D_A_real_loss: 0.104 D_A_fake_loss: 0.105 D_B_real_loss: 0.189 D_B_fake_loss: 0.185 \n",
      "{'pre_used': [3], 'enchain_used': [3, 2, 1, 0], 'dechain_used': [0, 1, 2, 3], 'post_used': [3]}\n",
      "{'inputs_used': [3], 'chain_used': [3, 4, 5, 6]}\n",
      "(current_level: 4.000, iters: 10000, time_from_last_report: 334.777, since_the_phase_start: 8355.929) G_gan_loss: 1.132 G_idt_loss: 0.958 G_cycle_loss: 1.318 D_A_real_loss: 0.111 D_A_fake_loss: 0.115 D_B_real_loss: 0.185 D_B_fake_loss: 0.181 \n",
      "{'pre_used': [3], 'enchain_used': [3, 2, 1, 0], 'dechain_used': [0, 1, 2, 3], 'post_used': [3]}\n",
      "{'inputs_used': [3], 'chain_used': [3, 4, 5, 6]}\n",
      "(current_level: 4.000, iters: 10400, time_from_last_report: 335.010, since_the_phase_start: 8690.943) G_gan_loss: 1.108 G_idt_loss: 0.978 G_cycle_loss: 1.333 D_A_real_loss: 0.116 D_A_fake_loss: 0.122 D_B_real_loss: 0.188 D_B_fake_loss: 0.180 \n",
      "{'pre_used': [3], 'enchain_used': [3, 2, 1, 0], 'dechain_used': [0, 1, 2, 3], 'post_used': [3]}\n",
      "{'inputs_used': [3], 'chain_used': [3, 4, 5, 6]}\n",
      "(current_level: 4.000, iters: 10800, time_from_last_report: 334.751, since_the_phase_start: 9025.695) G_gan_loss: 1.126 G_idt_loss: 0.940 G_cycle_loss: 1.352 D_A_real_loss: 0.112 D_A_fake_loss: 0.117 D_B_real_loss: 0.177 D_B_fake_loss: 0.171 \n",
      "{'pre_used': [3], 'enchain_used': [3, 2, 1, 0], 'dechain_used': [0, 1, 2, 3], 'post_used': [3]}\n",
      "{'inputs_used': [3], 'chain_used': [3, 4, 5, 6]}\n",
      "(current_level: 4.000, iters: 11200, time_from_last_report: 334.648, since_the_phase_start: 9360.341) G_gan_loss: 1.129 G_idt_loss: 0.983 G_cycle_loss: 1.322 D_A_real_loss: 0.114 D_A_fake_loss: 0.119 D_B_real_loss: 0.179 D_B_fake_loss: 0.172 \n",
      "{'pre_used': [3], 'enchain_used': [3, 2, 1, 0], 'dechain_used': [0, 1, 2, 3], 'post_used': [3]}\n",
      "{'inputs_used': [3], 'chain_used': [3, 4, 5, 6]}\n",
      "(current_level: 4.000, iters: 11600, time_from_last_report: 334.546, since_the_phase_start: 9694.896) G_gan_loss: 1.130 G_idt_loss: 0.958 G_cycle_loss: 1.340 D_A_real_loss: 0.113 D_A_fake_loss: 0.123 D_B_real_loss: 0.182 D_B_fake_loss: 0.176 \n",
      "{'pre_used': [3], 'enchain_used': [3, 2, 1, 0], 'dechain_used': [0, 1, 2, 3], 'post_used': [3]}\n",
      "{'inputs_used': [3], 'chain_used': [3, 4, 5, 6]}\n",
      "(current_level: 4.000, iters: 12000, time_from_last_report: 334.946, since_the_phase_start: 10029.868) G_gan_loss: 1.134 G_idt_loss: 0.959 G_cycle_loss: 1.348 D_A_real_loss: 0.118 D_A_fake_loss: 0.118 D_B_real_loss: 0.192 D_B_fake_loss: 0.187 \n",
      "{'pre_used': [3], 'enchain_used': [3, 2, 1, 0], 'dechain_used': [0, 1, 2, 3], 'post_used': [3]}\n",
      "{'inputs_used': [3], 'chain_used': [3, 4, 5, 6]}\n",
      "(current_level: 4.000, iters: 12400, time_from_last_report: 334.773, since_the_phase_start: 10364.652) G_gan_loss: 1.106 G_idt_loss: 0.975 G_cycle_loss: 1.352 D_A_real_loss: 0.113 D_A_fake_loss: 0.114 D_B_real_loss: 0.188 D_B_fake_loss: 0.188 \n",
      "{'pre_used': [3], 'enchain_used': [3, 2, 1, 0], 'dechain_used': [0, 1, 2, 3], 'post_used': [3]}\n",
      "{'inputs_used': [3], 'chain_used': [3, 4, 5, 6]}\n",
      "(current_level: 4.000, iters: 12800, time_from_last_report: 334.730, since_the_phase_start: 10699.385) G_gan_loss: 1.132 G_idt_loss: 0.978 G_cycle_loss: 1.346 D_A_real_loss: 0.110 D_A_fake_loss: 0.114 D_B_real_loss: 0.189 D_B_fake_loss: 0.182 \n",
      "{'pre_used': [3], 'enchain_used': [3, 2, 1, 0], 'dechain_used': [0, 1, 2, 3], 'post_used': [3]}\n",
      "{'inputs_used': [3], 'chain_used': [3, 4, 5, 6]}\n",
      "(current_level: 4.000, iters: 13200, time_from_last_report: 334.903, since_the_phase_start: 11034.304) G_gan_loss: 1.140 G_idt_loss: 1.007 G_cycle_loss: 1.348 D_A_real_loss: 0.120 D_A_fake_loss: 0.119 D_B_real_loss: 0.187 D_B_fake_loss: 0.186 \n",
      "{'pre_used': [3], 'enchain_used': [3, 2, 1, 0], 'dechain_used': [0, 1, 2, 3], 'post_used': [3]}\n",
      "{'inputs_used': [3], 'chain_used': [3, 4, 5, 6]}\n",
      "(current_level: 4.000, iters: 13600, time_from_last_report: 334.881, since_the_phase_start: 11369.187) G_gan_loss: 1.093 G_idt_loss: 0.966 G_cycle_loss: 1.338 D_A_real_loss: 0.124 D_A_fake_loss: 0.129 D_B_real_loss: 0.188 D_B_fake_loss: 0.189 \n",
      "{'pre_used': [3], 'enchain_used': [3, 2, 1, 0], 'dechain_used': [0, 1, 2, 3], 'post_used': [3]}\n",
      "{'inputs_used': [3], 'chain_used': [3, 4, 5, 6]}\n",
      "(current_level: 4.000, iters: 14000, time_from_last_report: 334.476, since_the_phase_start: 11703.670) G_gan_loss: 1.140 G_idt_loss: 1.013 G_cycle_loss: 1.335 D_A_real_loss: 0.121 D_A_fake_loss: 0.121 D_B_real_loss: 0.175 D_B_fake_loss: 0.176 \n",
      "{'pre_used': [3], 'enchain_used': [3, 2, 1, 0], 'dechain_used': [0, 1, 2, 3], 'post_used': [3]}\n",
      "{'inputs_used': [3], 'chain_used': [3, 4, 5, 6]}\n",
      "(current_level: 4.000, iters: 14400, time_from_last_report: 334.628, since_the_phase_start: 12038.316) G_gan_loss: 1.105 G_idt_loss: 0.961 G_cycle_loss: 1.327 D_A_real_loss: 0.113 D_A_fake_loss: 0.117 D_B_real_loss: 0.194 D_B_fake_loss: 0.186 \n",
      "{'pre_used': [3], 'enchain_used': [3, 2, 1, 0], 'dechain_used': [0, 1, 2, 3], 'post_used': [3]}\n",
      "{'inputs_used': [3], 'chain_used': [3, 4, 5, 6]}\n",
      "(current_level: 4.000, iters: 14800, time_from_last_report: 334.615, since_the_phase_start: 12372.927) G_gan_loss: 1.089 G_idt_loss: 0.982 G_cycle_loss: 1.318 D_A_real_loss: 0.116 D_A_fake_loss: 0.125 D_B_real_loss: 0.188 D_B_fake_loss: 0.194 \n",
      "{'pre_used': [3], 'enchain_used': [3, 2, 1, 0], 'dechain_used': [0, 1, 2, 3], 'post_used': [3]}\n",
      "{'inputs_used': [3], 'chain_used': [3, 4, 5, 6]}\n",
      "(current_level: 4.000, iters: 15200, time_from_last_report: 334.918, since_the_phase_start: 12707.863) G_gan_loss: 1.081 G_idt_loss: 0.966 G_cycle_loss: 1.324 D_A_real_loss: 0.116 D_A_fake_loss: 0.121 D_B_real_loss: 0.195 D_B_fake_loss: 0.190 \n",
      "{'pre_used': [3], 'enchain_used': [3, 2, 1, 0], 'dechain_used': [0, 1, 2, 3], 'post_used': [3]}\n",
      "{'inputs_used': [3], 'chain_used': [3, 4, 5, 6]}\n",
      "(current_level: 4.000, iters: 15600, time_from_last_report: 334.557, since_the_phase_start: 13042.434) G_gan_loss: 1.060 G_idt_loss: 0.968 G_cycle_loss: 1.312 D_A_real_loss: 0.121 D_A_fake_loss: 0.123 D_B_real_loss: 0.189 D_B_fake_loss: 0.190 \n",
      "{'pre_used': [3], 'enchain_used': [3, 2, 1, 0], 'dechain_used': [0, 1, 2, 3], 'post_used': [3]}\n",
      "{'inputs_used': [3], 'chain_used': [3, 4, 5, 6]}\n",
      "(current_level: 4.000, iters: 16000, time_from_last_report: 334.580, since_the_phase_start: 13377.024) G_gan_loss: 1.070 G_idt_loss: 0.960 G_cycle_loss: 1.309 D_A_real_loss: 0.124 D_A_fake_loss: 0.134 D_B_real_loss: 0.196 D_B_fake_loss: 0.195 \n",
      "{'pre_used': [3], 'enchain_used': [3, 2, 1, 0], 'dechain_used': [0, 1, 2, 3], 'post_used': [3]}\n",
      "{'inputs_used': [3], 'chain_used': [3, 4, 5, 6]}\n",
      "(current_level: 4.000, iters: 16400, time_from_last_report: 334.550, since_the_phase_start: 13711.568) G_gan_loss: 1.067 G_idt_loss: 0.968 G_cycle_loss: 1.307 D_A_real_loss: 0.124 D_A_fake_loss: 0.127 D_B_real_loss: 0.198 D_B_fake_loss: 0.193 \n",
      "{'pre_used': [3], 'enchain_used': [3, 2, 1, 0], 'dechain_used': [0, 1, 2, 3], 'post_used': [3]}\n",
      "{'inputs_used': [3], 'chain_used': [3, 4, 5, 6]}\n",
      "(current_level: 4.000, iters: 16800, time_from_last_report: 334.460, since_the_phase_start: 14046.050) G_gan_loss: 1.113 G_idt_loss: 0.967 G_cycle_loss: 1.313 D_A_real_loss: 0.113 D_A_fake_loss: 0.125 D_B_real_loss: 0.190 D_B_fake_loss: 0.185 \n",
      "{'pre_used': [3], 'enchain_used': [3, 2, 1, 0], 'dechain_used': [0, 1, 2, 3], 'post_used': [3]}\n",
      "{'inputs_used': [3], 'chain_used': [3, 4, 5, 6]}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(current_level: 4.000, iters: 17200, time_from_last_report: 334.966, since_the_phase_start: 14381.027) G_gan_loss: 1.076 G_idt_loss: 0.974 G_cycle_loss: 1.326 D_A_real_loss: 0.112 D_A_fake_loss: 0.119 D_B_real_loss: 0.193 D_B_fake_loss: 0.190 \n",
      "{'pre_used': [3], 'enchain_used': [3, 2, 1, 0], 'dechain_used': [0, 1, 2, 3], 'post_used': [3]}\n",
      "{'inputs_used': [3], 'chain_used': [3, 4, 5, 6]}\n",
      "(current_level: 4.000, iters: 17600, time_from_last_report: 336.028, since_the_phase_start: 14716.939) G_gan_loss: 1.113 G_idt_loss: 0.974 G_cycle_loss: 1.318 D_A_real_loss: 0.105 D_A_fake_loss: 0.120 D_B_real_loss: 0.191 D_B_fake_loss: 0.191 \n",
      "{'pre_used': [3], 'enchain_used': [3, 2, 1, 0], 'dechain_used': [0, 1, 2, 3], 'post_used': [3]}\n",
      "{'inputs_used': [3], 'chain_used': [3, 4, 5, 6]}\n",
      "(current_level: 4.000, iters: 18000, time_from_last_report: 334.270, since_the_phase_start: 15051.385) G_gan_loss: 1.104 G_idt_loss: 0.979 G_cycle_loss: 1.299 D_A_real_loss: 0.121 D_A_fake_loss: 0.132 D_B_real_loss: 0.197 D_B_fake_loss: 0.195 \n",
      "{'pre_used': [3], 'enchain_used': [3, 2, 1, 0], 'dechain_used': [0, 1, 2, 3], 'post_used': [3]}\n",
      "{'inputs_used': [3], 'chain_used': [3, 4, 5, 6]}\n",
      "(current_level: 4.000, iters: 18400, time_from_last_report: 333.590, since_the_phase_start: 15384.974) G_gan_loss: 1.091 G_idt_loss: 0.980 G_cycle_loss: 1.323 D_A_real_loss: 0.119 D_A_fake_loss: 0.124 D_B_real_loss: 0.197 D_B_fake_loss: 0.190 \n",
      "{'pre_used': [3], 'enchain_used': [3, 2, 1, 0], 'dechain_used': [0, 1, 2, 3], 'post_used': [3]}\n",
      "{'inputs_used': [3], 'chain_used': [3, 4, 5, 6]}\n",
      "(current_level: 4.000, iters: 18800, time_from_last_report: 333.589, since_the_phase_start: 15718.584) G_gan_loss: 1.099 G_idt_loss: 0.952 G_cycle_loss: 1.330 D_A_real_loss: 0.109 D_A_fake_loss: 0.117 D_B_real_loss: 0.182 D_B_fake_loss: 0.174 \n",
      "{'pre_used': [3], 'enchain_used': [3, 2, 1, 0], 'dechain_used': [0, 1, 2, 3], 'post_used': [3]}\n",
      "{'inputs_used': [3], 'chain_used': [3, 4, 5, 6]}\n",
      "(current_level: 4.000, iters: 19200, time_from_last_report: 333.368, since_the_phase_start: 16051.936) G_gan_loss: 1.237 G_idt_loss: 0.983 G_cycle_loss: 1.379 D_A_real_loss: 0.095 D_A_fake_loss: 0.104 D_B_real_loss: 0.183 D_B_fake_loss: 0.178 \n",
      "{'pre_used': [3], 'enchain_used': [3, 2, 1, 0], 'dechain_used': [0, 1, 2, 3], 'post_used': [3]}\n",
      "{'inputs_used': [3], 'chain_used': [3, 4, 5, 6]}\n"
     ]
    }
   ],
   "source": [
    "to_level = int(np.log2(target_resol))\n",
    "from_level = int(np.log2(first_resol))\n",
    "\n",
    "for model in [G_A,G_B,D_A,D_B]:\n",
    "    model.train()\n",
    "\n",
    "for R in range(from_level-1, to_level):\n",
    "    batch_size = bs_map[2 ** (R+1)]\n",
    "    phases = {'stabilize':[0, train_img//batch_size], 'fade_in':[train_img//batch_size+1, (transition_img+train_img)//batch_size]}\n",
    "    if R==to_level-1:\n",
    "        del phases['fade_in']\n",
    "    print('starting scale %d, batch_size is %d, and the iterations in each phase is %d'%(R,batch_size,train_img//batch_size))\n",
    "#     for phase in ['fade_in']: \n",
    "    for phase in ['stabilize', 'fade_in']:\n",
    "        num_pool = 80\n",
    "        fake_A_pool = util.ImagePool(num_pool)\n",
    "        fake_B_pool = util.ImagePool(num_pool)\n",
    "        if phase in phases:\n",
    "            _range = phases[phase]\n",
    "            from_it = _range[0]\n",
    "            cur_it = 0\n",
    "            total_it = _range[1]\n",
    "            remaining_it = total_it-from_it\n",
    "            cur_nimg = _range[0]*batch_size\n",
    "            resol = 2 ** (R+1)\n",
    "            \n",
    "            previous_time = time.time()\n",
    "            phase_start_time = time.time()\n",
    "            \n",
    "            for it in range(from_it, total_it):\n",
    "#             print(len(train_loader_A))\n",
    "#             print(len(train_loader_B))\n",
    "#             for it,((real_A,_),(real_B,_)) in enumerate(zip(train_loader_A, train_loader_B)):\n",
    "#                 set_trace()\n",
    "                cur_it += 1\n",
    "                if phase == 'stabilize':\n",
    "                    cur_level = R\n",
    "                else:\n",
    "                    cur_level = R + cur_it/remaining_it\n",
    "                cur_resol = 2 ** int(np.ceil(cur_level+1))\n",
    "                cur_nimg += batch_size\n",
    "                cur_scale = R+cur_it/total_it/2\n",
    "                if phase == 'fade_in':\n",
    "                    cur_scale+=0.5\n",
    "\n",
    "                real_A = data_A.next(batch_size,cur_resol*2,cur_level+1)             \n",
    "                real_B = data_B.next(batch_size,cur_resol*2,cur_level+1)\n",
    "                real_A,real_B = real_A.to(device), real_B.to(device)\n",
    "                # ===preprocess===\n",
    "#                 for param_group in optim_G.param_groups:\n",
    "#                     lrate_coef = _rampup(cur_nimg / 1000.0, rampup_kimg)\n",
    "#                     lrate_coef *= _rampdown_linear(cur_nimg / 1000.0,total_kimg, rampdown_kimg)\n",
    "#                     param_group['lr'] = lrate_coef * g_lr_max\n",
    "  \n",
    "#                 for param_group in optim_D_A.param_groups:\n",
    "#                     lrate_coef = _rampup(cur_nimg / 1000.0, rampup_kimg)\n",
    "#                     lrate_coef *= _rampdown_linear(cur_nimg / 1000.0, total_kimg, rampdown_kimg)\n",
    "#                     param_group['lr'] = lrate_coef * d_lr_max\n",
    "#                 for param_group in optim_D_B.param_groups:\n",
    "#                     lrate_coef = _rampup(cur_nimg / 1000.0, rampup_kimg)\n",
    "#                     lrate_coef *= _rampdown_linear(cur_nimg / 1000.0, total_kimg, rampdown_kimg)\n",
    "#                     param_group['lr'] = lrate_coef * d_lr_max\n",
    "                    \n",
    "                \n",
    "                # ===update G===\n",
    "                for model in [D_A,D_B]:\n",
    "                    for param in model.parameters():\n",
    "                        param.requires_grad = False\n",
    "                \n",
    "                #G_A\n",
    "                fake_B, G_model_status = G_A(real_A, cur_level)\n",
    "                d_fake_A, D_model_status = D_B(fake_B, cur_level)\n",
    "                d_fake_A_loss = GAN_loss(d_fake_A, torch.ones(d_fake_A.size(), device=device))\n",
    "                \n",
    "                sim_A_loss = L1_loss(fake_B,real_A) * lambda_idt\n",
    "                \n",
    "                recon_A,_ = G_B(fake_B,cur_level)\n",
    "                recon_A_loss = L1_loss(recon_A,real_A) * lambda_recon\n",
    "                \n",
    "                G_A_loss = (d_fake_A_loss+sim_A_loss+recon_A_loss)*lambda_A\n",
    "                \n",
    "                \n",
    "                fake_A,_ = G_B(real_B,cur_level)\n",
    "                d_fake_B,_ = D_A(fake_A,cur_level)\n",
    "                d_fake_B_loss = GAN_loss(d_fake_B, torch.ones(d_fake_B.size(), device=device))\n",
    "\n",
    "                sim_B_loss = L1_loss(fake_A,real_B)*lambda_idt\n",
    "                \n",
    "                recon_B,_ = G_A(fake_A,cur_level)\n",
    "                recon_B_loss = L1_loss(recon_B,real_B)*lambda_recon\n",
    "                \n",
    "                G_B_loss = (d_fake_B_loss + sim_B_loss + recon_B_loss)*lambda_B\n",
    "                \n",
    "                G_loss = G_A_loss+G_B_loss\n",
    "                \n",
    "                optim_G.zero_grad()\n",
    "                G_loss.backward()\n",
    "                optim_G.step()\n",
    "                \n",
    "                G_gan_loss = d_fake_A_loss + d_fake_B_loss\n",
    "                G_idt_loss = sim_A_loss + sim_B_loss\n",
    "                G_recon_loss = recon_A_loss + recon_B_loss\n",
    "\n",
    "                # ===generate sample images===\n",
    "                if it % show_it == 0:\n",
    "                    save_result = it % save_it == 0\n",
    "                    vis.display_current_results([real_A,fake_B,recon_A,real_B,fake_A,recon_B],cur_scale,save_result)\n",
    "                    \n",
    "#                 ===update D===\n",
    "                \n",
    "                for model in [D_A,D_B]:\n",
    "                    for param in model.parameters():\n",
    "                        param.requires_grad = True\n",
    "                #D_B\n",
    "                fake_B,_ = G_A(real_A, cur_level=cur_level)\n",
    "                fake_B_from_pool = fake_B_pool.query(fake_B.detach())\n",
    "                d_real_B,_ = D_B(real_B, cur_level=cur_level)\n",
    "                d_fake_B,_ = D_B(fake_B_from_pool, cur_level=cur_level)\n",
    "\n",
    "                d_real_B_loss = GAN_loss(d_real_B, torch.ones(d_real_B.size(), device=device))\n",
    "                d_fake_B_loss = GAN_loss(d_fake_B, torch.zeros(d_fake_B.size(), device=device))\n",
    "#                 d_real_B_loss = GAN_loss(d_real_B, 1-torch.rand(d_real_B.size(), device=device)/10.0)\n",
    "#                 d_fake_B_loss = GAN_loss(d_fake_B, torch.rand(d_real_B.size(), device=device)/10.0)        \n",
    "\n",
    "                d_loss_B = 0.5 * (d_real_B_loss + d_fake_B_loss)\n",
    "                \n",
    "                optim_D_B.zero_grad()\n",
    "                d_loss_B.backward()\n",
    "                optim_D_B.step()\n",
    "\n",
    "                #D_A\n",
    "                fake_A,_ = G_B(real_B, cur_level=cur_level)\n",
    "                fake_A_from_pool = fake_A_pool.query(fake_A.detach())\n",
    "                d_real_A,_ = D_A(real_A, cur_level=cur_level)\n",
    "                d_fake_A,_ = D_A(fake_A_from_pool, cur_level=cur_level)\n",
    "\n",
    "                d_real_A_loss = GAN_loss(d_real_A, torch.ones(d_real_A.size(), device=device))\n",
    "                d_fake_A_loss = GAN_loss(d_fake_A, torch.zeros(d_fake_A.size(), device=device))             \n",
    "#                 d_real_A_loss = GAN_loss(d_real_A, 1-torch.rand(d_real_B.size(), device=device)/10.0)\n",
    "#                 d_fake_A_loss = GAN_loss(d_fake_A, torch.rand(d_real_B.size(), device=device)/10.0)      \n",
    "\n",
    "                d_loss_A = 0.5 * (d_real_A_loss + d_fake_A_loss)\n",
    "    \n",
    "                optim_D_A.zero_grad()\n",
    "                d_loss_A.backward()    \n",
    "                optim_D_A.step()\n",
    "            \n",
    "            \n",
    "\n",
    "                # ===report ===\n",
    "                train_hist.add_params([G_gan_loss,G_idt_loss,G_recon_loss,d_real_A_loss,d_fake_A_loss,d_real_B_loss,d_fake_B_loss])\n",
    "                if it% report_it == 0:                    \n",
    "                    phase_time = time.time() - phase_start_time\n",
    "                    losses = train_hist.check_current_avg()\n",
    "                    it_time = time.time() - previous_time\n",
    "                    vis.print_current_losses(cur_level, it, losses, it_time, phase_time)\n",
    "                    vis.plot_current_losses(cur_scale, losses)\n",
    "                    previous_time = time.time()\n",
    "                    print(G_model_status)\n",
    "                    print(D_model_status)\n",
    "                    \n",
    "                \n",
    "                # ===save model===\n",
    "                if it % save_it == 0:\n",
    "                    save_models(all_models,project_dir)\n",
    "\n",
    "            model_folder_at_scale = os.path.join(project_dir,phase+str(R))\n",
    "            if not os.path.isdir(model_folder_at_scale):\n",
    "                os.makedirs(model_folder_at_scale)\n",
    "            save_models(all_models,model_folder_at_scale)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
